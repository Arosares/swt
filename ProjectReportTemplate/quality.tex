%\flushleft
\emph{(Approx.~4--7~pages of text.)}

\emph{Describe and justify the different quality assurance techniques that your group has applied alongside the project's conduct, including the INVEST criteria for the user stories, SMART criteria for the tasks derived from user stories, unit tests for your code, and others.  Illustrate your approach to quality assurance by giving relevant examples for each employed technique. Finally, do not forget to evaluate your software's interfaces (including the GUI).}
What did we do to ensure the quality of our final product? 

To ensure the development stayed on track and produced useable and useful code, our User Stories adhered to the INVEST criteria as much as practical, always detailing a specific feature either wanted by the client or seen as necessary by the Team for the inner workings of the program. 
However, we had to deviate from the template to adjust to the peculiarities of the project at hand:
The limited time of roughly 80 manhours total per sprint made it impractical to limit a userstory to "one third of a sprint"; Most often, a single sprint consisted of between two and three user stories, each of which spanned at least half a sprint to implement by between one and four people. 
The criteria and how closely we managed to follow them:

\newpage
\begin{table}[h]
  \caption{INVEST}
  \label{INVEST}
  \centering
  \begin{tabular}{|p{3cm} | p{9cm}|}
	\hline  	 
  	 Criteria & Comments \\ 
  	 \hline
  	 \hline
  	Independent & User Stories should be independent of each other if at all possible. Due to the nature of the project, some stories could not be fully independent: for Example, '\# 24 Display Analysis Data' would depend on actual data being present, as generated by code created in User Story \# 23. In general, all our UI-related stories require the results of work on the backend.  \\ 
  	\hline
  	 Negotiable & User Stories should describe what should be done, but not how. The details are to be decided upon in the tasks. We managed to adhere to this over all User Stories.  \\ 
  	\hline
  	Valuable & A User Story needs to specify a clear benefit to the customer. All our User Stories have a benefit to the customer, but in some cases, amongst other things our backend, the benefit may not be easily recognizable by the customer. Thus, some of our User Stories were not written from a customers perspective. \\ 
  	\hline
  	Estimable & A Story's size should always be estimable, whether by elaborate method or an educated guess. As we did have no experience in working as a team, we categorized stories roughly into three sizes: L, M, S.  Within that framework, our size estimates were relatively accurate.   \\ 
  	\hline
  	Small & User Stories should be no larger than one third of a sprint. Due to the limited amount of work time in a sprint, less than 20 hours per team member, we frequently did not manage to adhere to this principle. Early sprints only had two user stories each.  \\ 
  	\hline
  	Testable & To see if a User Story is complete, it desired result needs to be confirm-able by acceptance tests. Our stories generally have success criteria that can be tested against, though in the case of the GUI this was largely done by manual testing, as the team judged writing GUI tests to be too time-consuming to consider. \\ 
  	\hline
  \end{tabular}
\end{table}
\newpage
\paragraph{example user story card} 
\begin{table}[h]
  \caption{User Story 7: Testrun Table, Front}
  \label{Story 7 Example}
  \centering
  \begin{tabular}{|p{9cm} p{2cm}|}
	\hline  	
  	Testrun Table & Nr. 7.001  \\ 
  	\hline
  	As a user, &    \\ 
  	I want to display a table containing all the classes of one specific testrun and their failure percentage so that I can get a quick overview of the most problematic classes. &    \\ 
  	Size: M & Sprint: 2 \\ 
  	\hline
  \end{tabular}
\end{table}
\begin{table}[h]
  \caption{User Story 7: Testrun Table, Back}
  \label{Story 7 Example}
  \centering
  \begin{tabular}{|p{10cm} p{1cm}|}
  	  &    \\ 
  	The story is done when &    \\ 
  	 - the classes are sorted in decending order of failure percentage &    \\ 
  	 - the classes with the highest failure percentage are highlighted &   \\ 
  	 
  	  &  
  	   \\ 
  	\hline
  \end{tabular}
\end{table}

The Story above has an estimated size, a defined and testable goal, and is important to the project. How exactly is to be done is not part of the user story, instead being detailed in the respective tasks, or decided by the assigned team member at the time of implementation. 


When it came to Tasks, our team only gradually realized the practical importance of the "Specific" in the SMART-criteria; our early User Stories frequently only contained one or two Tasks, like "Implement Apriori". This became an apparent problem when we tried to improve our documentation in Sprint 4, and we found progress was very hard to track when two people worked on a single task for the majority of a sprint. 
Following this revelation, tasks were created based on what part of the code they concerned themselves with, like refactoring a single class, or a single function, for example the implementation of a distance calculation that spanned multiple classes but could still be linked to a single userstory and completed by a single teammember or a team of two. 
This also made the effort required for a single task more easily measurable and allowed us to track our progress in greater detail. A good example of this would be the difference between user stories 8 and 23, both concerning themselves with the implementation of the Apriori algorithm.

To ensure the working condition of our software, we employed extensive tests of internal methods wherever applicable. 
This includes all internal classes excepting the controller and model classes that frequently contain too many links to other classes, making testing them alone impractical. Most of the methods in those classes are indirectly tested through the tests of the classes they call upon. 


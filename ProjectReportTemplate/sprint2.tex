\subsection{Sprint No.~2}

\subsubsection*{Sprint Planning}

In Sprint 2 our goal was to display the information we draw out of the test run XML files during parsing in a graphical user interface, so that the user has an easy way to access the information we stored in the data structure of TDA. \\ 
Since we were not able to complete all user stories we chose for Sprint 1 our Sprint backlog for Sprint 2 contained four user stories. We still had to finish our work on user story \# 18 XML Parsing and we had not even started to implement user story \# 9 Test Run Selection. After some initial research on user story \# 9 and input from the client during the Sprint 1 review meeting we decided to create new tasks for it, which are more precise and easier to handle. Additionally two new user stories were planned for completion in this Sprint, user story \# 7 Test Run Table and \# 3 Test Run Chart. \\ 
The following tables give an overview on our Sprint backlog for Sprint 2. \\ 

\begin{table}[h]
  \caption{User Story number 18: XML Parsing}
  \label{US_Parsing2}
  \centering
  \begin{tabular}{p{1cm}|p{5cm}|p{3cm}|}
  	Nr & Description & Assigned \\ 
  	\hline
  	18.5 & Store parsed information in data structure & Frank, Tobias \\ 
  	\hline
  \end{tabular}
\end{table} 

\ \\

\begin{table}[h]
  \caption{User Story number 9: Test Run Selection}
  \label{US_Selection2}
  \centering
  \begin{tabular}{p{1cm}|p{5cm}|p{3cm}|}
  	Nr & Description & Assigned \\ 
  	\hline
  	9.21 & Create Main GUI window & Simon \\ 
  	\hline
  	9.22 & Create directory tree for test runs & Andreas \\ 
  	\hline
  	9.23 & Implement file browser to choose test runs & Jan \\
  	\hline
  	9.24 & Implement directory browser to choose root directory for XML file search & Andreas \\
  	\hline
  	9.25 & Test run overview & Jan \\ 
  	\hline
  \end{tabular}
\end{table} 

\ \\ 

\begin{table}[h]
  \caption{User Story number 7: Test Run Table}
  \label{US_Table}
  \centering
  \begin{tabular}{p{1cm}|p{5cm}|p{3cm}|}
  	Nr & Description & Assigned \\ 
  	\hline
  	7.1 & Calculate failure percentages of classes in test run & Frank, Tobias \\ 
  	\hline
  	7.2 & Implement a table to display all classes of one specific testrun and their failure percentages & Jan, Simon \\ 
  	\hline
  	7.3 & Apply sorting algorithm to sort classes by descending failure percentage & Jan, Simon, Tobias \\
  	\hline
  	7.4 & Highlight the class(es) with the highest failure percentage & Jan, Simon \\
  	\hline
  	7.5 & Event handling for testrun selection in directory tree to display corresponding table & Andreas, Frank \\ 
  	\hline
  \end{tabular}
\end{table} 

\ \\ 

\begin{table}[h]
  \caption{User Story number 3: Test Run Chart}
  \label{US_Chart}
  \centering
  \begin{tabular}{p{1cm}|p{5cm}|p{3cm}|}
  	Nr & Description & Assigned \\ 
  	\hline
  	3.1 & Classes sidebar overview &  \\ 
  	\hline
  	3.2 & Chart for displaying a single class and its failure percentage over all loaded testruns &  \\ 
  	\hline
  	3.3 & Event handling for class selection in sidebar overview to display corresponding chart &  \\
  	\hline
  \end{tabular}
\end{table} 

\ \\ 

\subsubsection*{Noteworthy Development Aspects}

In the Sprint review meeting for Sprint 1 we presented our so far implemented functionality to the client. We also presented our first paper prototype. Our design decisions were approved by the client and we could start to implement TDA's GUI accordingly. \\ 

During the presentation of our paper protype in which we presented the navigation through the system and its functionalities the client requested that we not only offer the possibility to select one or several files but also a method to select a root folder. All XML files in the selected folder and its subfolders should be parsed into our system. This request lead to the reworking of the tasks for user story \# 9. \\ 

The paper prototype was refined and extended, since it did not yet contain our first additional usage scenario. This additional scenario describes the possibility to compare the unit tests, their results and the failure percentage of a specific class in different test runs. We discussed how this scenario could be implemented and designed the paper prototype accordingly. \\ 

Additional artefacts that we reworked during this Sprint were the high level architecture diagram and our use case diagram. \\ 
The high level architecture diagram was designed to help us to visualise the Model View Controller pattern we chose for our system in the beginning of the project. It was also used to help us decide which classes and functions belong to which part of the TDA. \\ 
The use case diagram was intended to help us identify the different actors that would interact with the system. We also hoped to better understand the relationships and dependencies between the funcionalities of the TDA. \\ 

Both diagrams were presented to Ms. Sediki during the Mid Sprint Meeting. After her feedback we needed to refine the high level architecture diagram to better represent our so far implemented system. The use case diagram also needed to be reworked. During the reworking of the use case diagram we realised that all functionalities the TDA offers depend on the selection of test run XML files that should be parsed and analysed. This led to the fact that the first thing the user has to do after starting the TDA is to select one or several files, or a root folder containing test run XML files, that should be parsed. So we designed a new GUI window that offers the user only the two functionalities to either select one or several files or to select a folder. After the parsing has finished and its information is stored in TDA's data structure the user can then access the other implemented functionalities.

\subsubsection*{Sprint Review}

Our goal for Sprint 2 was not fully reached. We did not implement the test run chart, documented in user story \# 3. We were able to finish user story \# 18 XML Parsing. The only task of user story \# 7 Test Run Table was the highlighting of the classes with the highest failure percentages. \\ 

After all we were able to present the client a working GUI in which it is possible to select and parse test run files. The parsed test runs are displayed in a directory tree view and can be selected to display the corresponding table containing all its classes and their failure percentages. The table is also sorted in descending order of failure percentage as requested. The GUI also displays a summary of the selected test run which shows the total number of unit tests, the number of passed and failed unit tests, etc. \\ 

During this Sprint the work flow in our group was improved by the fact that parallel work was much more possible, since the dependencies between the different tasks were reduced. \\ 

Several merge conflicts and resulting errors in our system had cost us a lot of time. To reduce the merge conflicts we decided to create more branches and make sure to not work on the same functionalities or in the same classes in parallel. \\ 

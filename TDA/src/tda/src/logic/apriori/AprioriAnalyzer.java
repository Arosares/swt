package tda.src.logic.apriori;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map.Entry;
import java.util.Set;
import java.util.stream.Collectors;

import org.omg.CORBA.FREE_MEM;

import com.sun.javafx.image.impl.ByteIndexed.Getter;

import tda.src.logic.Analyzer;
import tda.src.logic.TestData;
import tda.src.logic.TestRun;
import tda.src.logic.TestedClass;

/**
 * @author Tobias Schwartz
 * @version 2017-01-17
 *
 */
public class AprioriAnalyzer implements Analyzer {

	private int minSupport;
	private double minConfidence;

	// A list containing all testedClasses
	List<TestedClass> items = new LinkedList<TestedClass>();
	// A list containing all TestRuns
	List<TestRun> transactions = new LinkedList<TestRun>();

	// All item sets with a set (list) of testedClasses as key and its support
	// count as value
	HashMap<List<TestedClass>, Integer> itemSet;

	// -- CACHE --
	// Stores all item sets and rules for all test runs
	// Filtering can then be done fast over cached entries
	List<StrongRule> cachedStrongRules = new LinkedList<>();
	HashMap<List<TestedClass>, Integer> cachedFrequentItemSets = new HashMap<>();

	private final int failedPercentage = 15;

	/**
	 * Creates a new {@link AprioriAnalyzer} with 
	 * support = number of transactions / 2
	 * confidence = 80 %
	 */
	public AprioriAnalyzer() {
		newAprioriAnalyzer(0.8);
	}
	
	public AprioriAnalyzer(double minConfidence) {
		newAprioriAnalyzer(minConfidence);
	}

	private void newAprioriAnalyzer(double minConfidence) {
		itemSet = new HashMap<List<TestedClass>, Integer>();

		this.minConfidence = minConfidence;
	}

	@Override
	public void analyze() {
		items = TestData.getInstance().getTestedClasses();
		transactions = TestData.getInstance().getTestRunList();
		
		minSupport = transactions.size() / 2;
		
		System.out.println("Starting Apriori with minimum support of " + this.minSupport);

		// 1. Generate frequent item sets
		System.out.println("\nMaximal Frequent Item Sets:");
		HashMap<List<TestedClass>, Integer> frequentItemSet = getMaxFrequentItemSet(2);
		printItemSet(frequentItemSet);

		// 2. Generate strong rules from frequent item sets
		System.out.println("\nStrong Rules");
//		for(int dist = 2; dist < 8; dist++){
		List<StrongRule> strongRules = generateStrongRules(frequentItemSet, 2);
//		}
		// 3. Update Cache
		cachedFrequentItemSets = frequentItemSet;
		cachedStrongRules = strongRules;
		
		strongRules = getStrongRules(minConfidence); 
		for (StrongRule strongRule : strongRules) {
			strongRule.print();
		}
	}
	
	/* ** Cache Functionality ** */
	
	
	/**
	 * Retrieves all strong rules above the given confidence.
	 * Uses cached results, if possible.
	 * 
	 * @param confidence
	 * 		confidence value between 0 and 1
	 * @return
	 * 		{@link List} of {@link StrongRule}s
	 */
	public List<StrongRule> getStrongRules(double confidence) {
		if (cachedStrongRules.isEmpty()) {
			analyze();
		}
		return cachedStrongRules
				.stream()
				.parallel()
				.filter(rule -> rule.getConfidence() >= confidence)
				.sorted()
				.collect(Collectors.toList());
	}
	
	/**
	 * Retrieves all strong rules above the given confidence,
	 * but with a class distance smaller or equal than the given distance.
	 * Uses cached results, if possible.
	 * 
	 * @param confidence
	 * 		confidence value between 0 and 1
	 * @param distance
	 * 		maximum distance between the classes of the {@link StrongRule}
	 * @return
	 * 		{@link List} of {@link StrongRule}s
	 */
	public List<StrongRule> getStrongRules(double confidence, int distance) {
		if (cachedStrongRules.isEmpty()) {
			analyze();
		}
		
		return cachedStrongRules
				.stream()
				.parallel()
				.filter(rule -> rule.getConfidence() >= confidence)
				.filter(rule -> rule.getMaxDistance() <= distance)
				.sorted()
				.collect(Collectors.toList());
	}
	
	/**
	 * Retrieves the maximal frequent item sets generated by the Apriori algorithm.
	 * Uses cached results, if possible.
	 * 
	 * @return
	 * 		The maximal frequent item set using a {@link HashMap}
	 */
	public HashMap<List<TestedClass>, Integer> getFrequentItemSets() {
		if (cachedFrequentItemSets.isEmpty()) {
			analyze();
		}
		return cachedFrequentItemSets;
	}
	
	public HashMap<List<TestedClass>, Integer> getFrequentItemSets(int distance) {
		if (cachedFrequentItemSets.isEmpty()) {
			analyze();
		}
	
		HashMap<List<TestedClass>, Integer> result = new HashMap<>();
	
		for (Entry<List<TestedClass>, Integer> entry : cachedFrequentItemSets.entrySet()) {
			if (getMaxDistance(entry.getKey()) <= distance) {
				result.put(entry.getKey(), entry.getValue());
			}
		}
		return result;
	}
	
	/* -------------------------------------------------------
	EVERYTHING BELOW IS INTERNAL COMPUTATION
	   ------------------------------------------------------- */
	
	private int getMaxDistance(List<TestedClass> testedClasses) {
		if (testedClasses.size() < 2)
			return 0;

		int maxDistance = 0;

		for (int i = 0; i < testedClasses.size() - 1; i++) {
			for (int j = i + 1; j < testedClasses.size(); j++) {
				int distance = TestData.getInstance().getClassDistance(testedClasses.get(i), testedClasses.get(j));
				if (distance > maxDistance) {
					maxDistance = distance;
				}
				System.out.println("Class " + testedClasses.get(i) + " , " + testedClasses.get(j) + " Distance: " + maxDistance);
			}
		}
		
		return maxDistance;
	}
	

	/**
	 * Generates the strong rules from a given frequent item set
	 * 
	 * @param frequentItemSet
	 *            The frequent item set used for generation of strong rules
	 * @return A {@link list} of all possible {@link StrongRule}s
	 */
	private List<StrongRule> generateStrongRules(HashMap<List<TestedClass>, Integer> frequentItemSet, int distanceCap) {

		List<StrongRule> strongRules = new LinkedList<>();

		for (Entry<List<TestedClass>, Integer> entry : frequentItemSet.entrySet()) {
			List<TestedClass> entryKey = entry.getKey();

			HashMap<List<TestedClass>, Integer> powerItemSet = getPowerSet(entryKey, distanceCap);
			updateItemSet(powerItemSet);

			strongRules.addAll(generateStrongRulesForEntry(powerItemSet, entryKey));
		}

		return strongRules;
	}

	/**
	 * Generates the strong rules for the powerset of one item set entry
	 * 
	 * @param powerItemSet
	 * 			The powerset of the given key 
	 * @param fullKey
	 * 			The key itself
	 * @return
	 * 			A List of this entry's {@link StrongRule}s
	 */
	private List<StrongRule> generateStrongRulesForEntry(HashMap<List<TestedClass>, Integer> powerItemSet,
			List<TestedClass> fullKey) {
		List<StrongRule> strongRules = new LinkedList<>();

		// S -> (I - S)
		for (Entry<List<TestedClass>, Integer> entry : powerItemSet.entrySet()) {
			List<TestedClass> entryKey = entry.getKey();
			if (entryKey.size() >= 1 && entryKey.size() < fullKey.size()) {
				List<TestedClass> leftSide = entryKey;
				List<TestedClass> rightSide = new LinkedList<>();
				rightSide.addAll(fullKey);
				rightSide.removeAll(leftSide);

				// conf = support (I) / support (S)
				double confidence = (double) powerItemSet.get(fullKey) / entry.getValue();
				StrongRule strongRule = new StrongRule(leftSide, rightSide, confidence);
				strongRules.add(strongRule);
			}
		}
		return strongRules;
	}

	private HashMap<List<TestedClass>, Integer> getPowerSet(List<TestedClass> testedClasses, int distanceCap) {
		HashMap<List<TestedClass>, Integer> powerItemSet = new HashMap<>();
		for (int i = 1; i <= testedClasses.size(); i++) {
			powerItemSet.putAll(generateFixedSubset(i, testedClasses, distanceCap));
		}
		return powerItemSet;
	}

	/**
	 * Generates the maximal frequent item set by steadily increasing length k. Item
	 * sets below the minimum support are pruned and therefore not used in
	 * further computations.
	 * 
	 * @return The maximal frequent item set
	 */
	//TODO: We need to create multiple sets based on potential Distance settings
	private HashMap<List<TestedClass>, Integer> getMaxFrequentItemSet(int distanceCap) {
		// Generate first item set as basis for the algorithm
		HashMap<List<TestedClass>, Integer> itemSet = getFirstItemSet();
		HashMap<List<TestedClass>, Integer> frqItemSet = pruneItemSet(itemSet, minSupport);

		// Helper to keep track of the last non empty frequent item set
		HashMap<List<TestedClass>, Integer> oldFrqItemSet = new HashMap<List<TestedClass>, Integer>();

		// Max frequent item set - to be returned
		HashMap<List<TestedClass>, Integer> maxFrequentItemSet = new HashMap<List<TestedClass>, Integer>();

		// length of item set
		int k = 1;

		while (!frqItemSet.isEmpty()) {
			oldFrqItemSet = frqItemSet;
			
			System.out.println("Looking at length " + k);
			printItemSet(frqItemSet);
			itemSet = initializeNewItemSet(frqItemSet, k + 1, distanceCap);
			
			updateItemSet(itemSet);
			
			printItemSet(itemSet);

			itemSet = pruneItemSet(itemSet, minSupport);
			frqItemSet = eliminateInsignificantItems(itemSet, oldFrqItemSet, distanceCap);
			
			
			/*
			* if an entry of the old item set is not a subset of any new frq item set
			* (i.e. possible rule but no further correlations)
			* and its key size is greater than 2 (i.e. a rule can be obtained)
			* then add this entry to the max frequent item set
			*/
			for (Entry<List<TestedClass>, Integer> entry : oldFrqItemSet.entrySet()) {
				if (entry.getKey().size() >= 2 && !isSubset(entry, frqItemSet)) {
					maxFrequentItemSet.putIfAbsent(entry.getKey(), entry.getValue());
				}
			}
			
			k++;
		}

		return maxFrequentItemSet;
	}
	
	
	/**
	 * An entry e is the subset of another item set I, 
	 * if there exists at least one entry e' in I
	 * so that e' contains all elements of e.
	 *    
	 * @return
	 * 		true if e is subset of I
	 */
	private boolean isSubset(Entry<List<TestedClass>, Integer> entry, HashMap<List<TestedClass>, Integer> itemSet) {
		return itemSet.keySet().stream().anyMatch(key -> key.containsAll(entry.getKey()));
	}

	/**
	 * Eliminates the insignificant items from the given item set by creating a new item set with only the significant items.
	 * An item is significant, if all possible items with key length - 1 were included in the previous iteration
	 * (i.e. are part of the previous item set)
	 *  
	 * @param itemSet
	 * 		item set to be filtered
	 * @param oldItemSet
	 * 		item set of the previous iteration
	 * @return
	 * 		item set containing only significant items
	 */
	private HashMap<List<TestedClass>, Integer> eliminateInsignificantItems(
			HashMap<List<TestedClass>, Integer> itemSet, HashMap<List<TestedClass>, Integer> oldItemSet, int distanceCap) {
		HashMap<List<TestedClass>, Integer> result = new HashMap<>();

		for (Entry<List<TestedClass>, Integer> entry : itemSet.entrySet()) {
			HashMap<List<TestedClass>, Integer> subset = generateFixedSubset(entry.getKey().size() - 1, entry.getKey(), distanceCap);
			if (subset.entrySet().stream().allMatch(e -> oldItemSet.containsKey(e.getKey()))) {
				result.put(entry.getKey(), entry.getValue());
			}
		}
		return result;
	}

	/**
	 * Analyzes the support count of the given item set.
	 * Important notice: The algorithm works directly on the given item set.
	 * Hence, the item set is overwritten by this algorithm.
	 * This allows for usage without any return value.
	 *  
	 * @param itemSet
	 * 		The item set to be (re)analyzed
	 */
	private void updateItemSet(HashMap<List<TestedClass>, Integer> itemSet) {
		itemSet.values().parallelStream().map(v -> v = 0);
		
		for (TestRun testRun : transactions) {
			for (Entry<List<TestedClass>, Integer> entry : itemSet.entrySet()) {
				if (getFailedClasses(testRun).containsAll(entry.getKey())) {
					incrementItemSetValue(itemSet, entry.getKey());
				}
			}
		}
	}

	/**
	 * Uses the given item set to generate a new item set with all possible key combinations with length k.   
	 * Entry value is 0
	 * 
	 * @param itemSet
	 * 		the item set the algorithm is based on
	 * @param length
	 * 		the required key length k 
	 * @return
	 */
	private HashMap<List<TestedClass>, Integer> initializeNewItemSet(HashMap<List<TestedClass>, Integer> itemSet,
			int length, int distanceCap) {
		System.out.println("WEED");
		System.out.println(itemSet);
		List<TestedClass> allTestedClasses = extractTestedClasses(itemSet);
		System.out.println("OI!");
		System.out.println(allTestedClasses);
		return generateFixedSubset(length, allTestedClasses, distanceCap);
	}

	/**
	 * Extracts a {@link List} of all {@link TestedClass}es contained in the given item set
	 * @param itemSet
	 * 		Item set used for extraction
	 * @return
	 * 		A {@link List} of all {@link TestedClass}es contained in the item set   
	 */
	private List<TestedClass> extractTestedClasses(HashMap<List<TestedClass>, Integer> itemSet) {
		List<TestedClass> result = new LinkedList<TestedClass>();
		for (List<TestedClass> testedClasses : itemSet.keySet()) {
			for (TestedClass testedClass : testedClasses) {
				if (!result.contains(testedClass)) {
					result.add(testedClass);
				}
			}
		}
		return result;
	}

	/**
	 * Removes all items not satisfying the given minimum support.
	 * 
	 * @param itemSet
	 * @param minSupport
	 * @return
	 * 		Item set with all items with value >= minimum support
	 */
	private HashMap<List<TestedClass>, Integer> pruneItemSet(HashMap<List<TestedClass>, Integer> itemSet,
			int minSupport) {
		HashMap<List<TestedClass>, Integer> frqItemSet = new HashMap<>();
		for (Entry<List<TestedClass>, Integer> entry : itemSet.entrySet()) {
			if (entry.getValue() >= Integer.valueOf(minSupport)) {
				frqItemSet.put(entry.getKey(), entry.getValue());
			}
		}
		return frqItemSet;
	}

	/**
	 * Maps every TestRun to its TestedClasses with a FP higher then a certain
	 * bound (field: failurePercentage)
	 * 
	 * @return HasMap, containing one List for each TestRun containing the
	 *         selected TestedClasses
	 */
	private HashMap<List<TestedClass>, Integer> getFirstItemSet() {
		return transactions.stream().map(testRun -> getFailedClasses(testRun)).collect(HashMap::new,
				(itemSet, testedClass) -> addTestedClassesToFirstItemSet(itemSet, testedClass), HashMap::putAll);
	}

	private void addTestedClassesToFirstItemSet(HashMap<List<TestedClass>, Integer> itemSet,
			List<TestedClass> testedClasses) {
		for (TestedClass testedClass : testedClasses) {
			addTestedClassToItemSet(itemSet, testedClass);
		}
	}

	private void addTestedClassToItemSet(HashMap<List<TestedClass>, Integer> itemSet, TestedClass testedClass) {
		List<TestedClass> testedClassList = new LinkedList<TestedClass>();
		testedClassList.add(testedClass);
		incrementItemSetValue(itemSet, testedClassList);
	}

	private void incrementItemSetValue(HashMap<List<TestedClass>, Integer> itemSet, List<TestedClass> key) {
		Integer value = itemSet.getOrDefault(key, 0);
		itemSet.put(key, value + 1);
	}

	/**
	 * Helper function to output an item set on the console
	 * @param itemSet
	 * 		item set to be printed
	 */
	private void printItemSet(HashMap<List<TestedClass>, Integer> itemSet) {
		Set<List<TestedClass>> keySet = itemSet.keySet();
		for (List<TestedClass> key : keySet) {
			int counter = 0;
			System.out.print("[");
			for (TestedClass testedClass : key) {
				System.out.print(testedClass.getClassName());
				if (counter < key.size() - 1) {
					System.out.print(", ");
					counter ++;
				}
			}
			System.out.print("] = " + itemSet.get(key) + "\n");
		}
		System.out.println();
	}

	private List<TestedClass> getFailedClasses(TestRun testRun) {
		return getFailedClasses(failedPercentage, testRun);
	}

	/**
	 * @param failurePercentage
	 *            the min FP a class should have in the List
	 * @param testRun
	 *            The TestRun which will be searched for classes
	 * @return List of TestedClasses containing only classes with higher FP as
	 *         the param failurePercentage
	 */
	private List<TestedClass> getFailedClasses(double failurePercentage, TestRun testRun) {
		return items.stream().parallel()
				.filter(testedClass -> testedClass.getFailurePercentageByTestrun(testRun) > failurePercentage)
				.collect(Collectors.toList());
	}

	private HashMap<List<TestedClass>, Integer> generateFixedSubset(int length, List<TestedClass> allTestedClasses, int distanceCap) {
		// generating all possible combinations with length k
		// based on: http://stackoverflow.com/questions/29910312/algorithm-to-get-all-the-combinations-of-size-n-from-an-array-java

		HashMap<List<TestedClass>, Integer> newItemSet = new HashMap<>();

		int[] indices = new int[length];

		if (length <= allTestedClasses.size()) {
			// first index sequence: 0, 1, 2, ...
			for (int i = 0; (indices[i] = i) < length - 1; i++)
				;
//			TODO: 
			System.out.println("WAT?");
			System.out.println(allTestedClasses.toString());
			newItemSet.put(getSubset(allTestedClasses, indices), 0);
			for (;;) {
				int i;
				// find position of item that can be incremented
				for (i = length - 1; i >= 0 && indices[i] == allTestedClasses.size() - length + i; i--)
					;
				if (i < 0) {
					break;
				} else {
					indices[i]++; // increment this item
					for (++i; i < length; i++) { // fill up remaining items
						indices[i] = indices[i - 1] + 1;
					}
//					if (TestData.getInstance().getClassDistance(classLeft, classRight))
					System.out.println("Subset to be put: ");
					System.out.println(getSubset(allTestedClasses, indices));
					newItemSet.put(getSubset(allTestedClasses, indices), 0);
				}
			}
		}
		return newItemSet;
	}

	// generate actual subset by index sequence
	List<TestedClass> getSubset(List<TestedClass> allTestedClasses, int[] subset) {
		List<TestedClass> result = new ArrayList<>(subset.length);
		System.out.println(allTestedClasses);
		for (int i = 0; i < subset.length; i++) {
//			if(TestData.getInstance().getClassDistance(allTestedClasses.get(subset[i])))
			result.add(i, allTestedClasses.get(subset[i]));
			System.out.println("Result.add: ");
			System.out.println(result);
		}
		System.out.println("results in: ");
		System.out.println(result);
		if(TestData.getInstance().getClassDistance(result.get(0),result.get(1))<=4){
			System.out.println("Distance sufficiently small");
			}else{
			result.clear();
		}
		return result;
	}
}
